{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "from Rules_Class import Rules\n",
    "import functions as fn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "from config import output_directory\n",
    "from config import input_directory\n",
    "from config import result_directory\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part=sys.argv[1]\n",
    "#part='part0'\n",
    "trrust_raw=pd.read_csv(output_directory+input_name+\".csv\",sep='\\t',header=(0),dtype=object)\n",
    "\n",
    "trrust_raw_result=pd.DataFrame(columns=['SrcID','SrcName','TrgID','TrgName','Mode','PMID','output','rank','status','evi_pmid','evi_sent'])\n",
    "\n",
    "\n",
    "Positive=[]\n",
    "[Positive.append(line.strip().upper()) for line in open(input_directory+\"Positive.txt\")]\n",
    "Negative=[]\n",
    "[Negative.append(line.strip().upper()) for line in open(input_directory+\"Negative.txt\")]\n",
    "\n",
    "genes_ents=input_directory + \"ALL_Human_Genes_Info.csv\" #NCBI\n",
    "genes=pd.read_csv(genes_ents,sep=',',header=(0))\n",
    "genes.fillna('', inplace=True)\n",
    "\n",
    "lookup_ids=pd.read_csv(input_directory+\"ncbi_id_lookup.csv\",sep='\\t',header=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row in trrust_raw.itertuples():\n",
    "    #try:\n",
    "        query_id=[int(row.SrcID),int(row.TrgID)] #NCBI ID MUST [TF,Target]\n",
    "        query_genes=[row.SrcName,row.TrgName] #Symbol MUST [TF,Target]\n",
    "        print(query_genes)\n",
    "        PubIDs =row.PMID.split(';')\n",
    "        query_genes, query_id,single_gene,single_id =fn.make_query(genes,lookup_ids,query_genes,query_id)\n",
    "        myterm=fn.term_maker(single_gene,genes)\n",
    "        Entrez.email=\"saman.farahmand001@umb.edu\"\n",
    "        \n",
    "        if(len(PubIDs)>0):\n",
    "            sum_ranks=[]\n",
    "            for PubID in PubIDs:\n",
    "                status=''\n",
    "                ranks=[]\n",
    "                annot_df=pd.DataFrame(columns=['type','id','text','offset','length'])\n",
    "                try:\n",
    "                    annot_df, abstract=fn.pubtator_annot(annot_df,PubID)\n",
    "                except:\n",
    "                    abstract=fn.ret_abstract(PubID)\n",
    "                    if(abstract=='?'):\n",
    "                        print(\"PMID=[\"+PubID+\"] does not exist any more!\")\n",
    "                        continue # remove it from the output results in TRRUST\n",
    "                    else:\n",
    "                        status+=\"PMID=[\"+PubID+\"] PubTator Response is not readable, Try to annotate manually...\"\n",
    "                        print(status)\n",
    "                    #add surface similarity\n",
    "                try:\n",
    "                    beCAS_lookup_full=fn.beCAS_lookup(PubID,query_id)\n",
    "                    beCAS_lookup=beCAS_lookup_full[['type','id','text','offset','length']]\n",
    "                    annot_df=pd.concat([annot_df,beCAS_lookup], ignore_index=True)\n",
    "                except:\n",
    "                    print(\"beCAS Server error...!\")\n",
    "\n",
    "                lookup_results=fn.lookup_annot(abstract,query_genes,query_id,lookup_ids)\n",
    "                annot_df=annot_df.append(lookup_results)\n",
    "                surface_annot=fn.surface_similarity(abstract, genes, query_genes, query_id,lookup_ids,single_id)\n",
    "                annot_df=annot_df.append(surface_annot)\n",
    "                annot_df=annot_df.drop_duplicates(subset=['id','offset'])\n",
    "\n",
    "                annot_df=fn.multiple_taxonomy(annot_df, query_id)\n",
    "\n",
    "\n",
    "                annot_df=annot_df.reset_index(drop=True)\n",
    "                candidate_sentences, covered=fn.candidate_sentence(annot_df,abstract,query_id)\n",
    "                if(len(candidate_sentences.index)==0):\n",
    "                    status+=\"PMID=[\"+PubID+\"] No co-existed sentences found in the abstract...!\"\n",
    "                    print(status)\n",
    "                    continue\n",
    "                evi_sentence=[]\n",
    "                evi_pmids=[]\n",
    "                for sentence in candidate_sentences.itertuples():\n",
    "                    obj=Rules(Positive,Negative,annot_df,covered,abstract,query_genes,query_id,sentence)\n",
    "                    depgraphs=fn.dep_parser('9000',sentence,annot_df,query_id,single_id,Positive,Negative,2)\n",
    "                    if(depgraphs):\n",
    "                        try:\n",
    "                            obj. multiplication_score(depgraphs, single_id)\n",
    "                        except:\n",
    "                            status+=\"PMID=[\"+PubID+\"] dependency graph score error...!\"\n",
    "                    else:\n",
    "                        status+=\"PMID=[\"+PubID+\"] dependency graph co-occurance of single ids error...!\"\n",
    "                        continue\n",
    "                    #obj.search_ranking()\n",
    "                    ranks.append(obj.rank)\n",
    "                    if(obj.rank!=0):\n",
    "                        evi_sentence.append('['+PubID+']'+sentence.sentence)\n",
    "                        evi_pmids.append(PubID)\n",
    "                if(len(ranks)!=0):\n",
    "                    sum_ranks.append(sum(ranks))\n",
    "            mode=''\n",
    "            rank_T=sum(sum_ranks)\n",
    "            if(rank_T>0):\n",
    "                mode='positive'\n",
    "            if(rank_T<0):\n",
    "                mode='negative'\n",
    "            evi_sentence=';'.join(evi_sentence)\n",
    "            evi_pmids=';'.join(evi_pmids)\n",
    "            trrust_raw_result=trrust_raw_result.append({'SrcID':row.SrcID,'SrcName':row.SrcName,'TrgID':row.TrgID,'TrgName':row.TrgName,'Mode':row.Mode,'PMID':row.PMID,'output':mode,'rank':str(rank_T),'status':status,'evi_pmid':evi_pmids,'evi_sent':evi_sentence},ignore_index=True)\n",
    "        else:\n",
    "            print(\"Not found any PMIDs for this interaction...!\")\n",
    "            trrust_raw_result=trrust_raw_result.append({'SrcID':row.SrcID,'SrcName':row.SrcName,'TrgID':row.TrgID,'TrgName':row.TrgName,'Mode':row.Mode,'PMID':row.PMID,'output':'error','rank':float('nan'),'status':'Not found any PMIDs for this interaction','evi_pmid':'','evi_sent':''},ignore_index=True)\n",
    "    #except:\n",
    "        #print(\"general Error!!\")\n",
    "        #continue\n",
    "\n",
    "trrust_raw_result.to_csv(result_directory+part+\"-result.csv\",sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###confusion matrix\n",
    "trrust_raw_result=pd.read_csv(result_directory+\"run1_trrust_raw_result.csv\", sep=\"\\t\",header=(0),dtype=object)\n",
    "trrust_raw_result.fillna('', inplace=True)\n",
    "trrust_not_detected=trrust_raw_result[(trrust_raw_result[\"status\"]!='') & (trrust_raw_result['output']=='')]\n",
    "trrust_detected=trrust_raw_result[(trrust_raw_result[\"status\"]=='') & (trrust_raw_result['output']!='')]\n",
    "print('Number of not detected (co-occurance) enteries: ' + str(len(trrust_not_detected.index)))\n",
    "print('Number of detected (co-occurance) enteries: ' + str(abs(len(trrust_raw_result.index)-len(trrust_not_detected.index))) + ' out of ' + str(len(trrust_raw_result.index)))\n",
    "identified_with_mode=trrust_detected[trrust_detected['output']!='']\n",
    "print('Number of detected enteries with mode of regulation: ' + str(len(identified_with_mode.index)))\n",
    "test=identified_with_mode[\"Mode\"].tolist()\n",
    "pred=identified_with_mode[\"output\"].tolist()\n",
    "cnf_matrix = confusion_matrix(test, pred)\n",
    "print(cnf_matrix)\n",
    "print(\"accuracy considering identified interactions: \" + str((cnf_matrix[0][0]+cnf_matrix[1][1])/len(identified_with_mode.index)*100))\n",
    "print(\"accuracy considering all interactions: \" + str((cnf_matrix[0][0]+cnf_matrix[1][1])/len(trrust_raw_result.index)*100))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
